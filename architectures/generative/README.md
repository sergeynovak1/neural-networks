# Генеративные модели

## Описание

Генеративные модели - это класс машинного обучения, который учится моделировать распределение данных с целью генерации новых примеров, похожих на обучающие данные. В отличие от дискриминативных моделей, которые учатся разделять классы, генеративные модели стремятся понять, как данные "устроены".

## Основные типы генеративных моделей

### 1. Генеративно-состязательные сети (GAN)
- Состоят из двух сетей: генератора и дискриминатора
- Обучаются в процессе "состязания" друг с другом
- Генератор создает поддельные данные, дискриминатор пытается их отличить от реальных

### 2. Вариационные автоэнкодеры (VAE)
- Вероятностный подход к автоэнкодерам
- Кодируют данные в вероятностное распределение
- Могут генерировать новые данные путем сэмплирования из латентного пространства

### 3. Нормализующие потоки (Normalizing Flows)
- Преобразуют простое распределение в сложное через последовательность обратимых преобразований
- Позволяют точно вычислять плотность вероятности

### 4. Автогрессивные модели
- Моделируют совместное распределение как произведение условных распределений
- Генерируют данные поэлементно (например, PixelRNN, WaveNet)

## GAN: Архитектура и принцип работы

### Компоненты:
1. **Генератор** - создает поддельные данные из случайного шума
2. **Дискриминатор** - отличает реальные данные от поддельных

### Процесс обучения:
1. Генератор создает поддельные данные
2. Дискриминатор получает как реальные, так и поддельные данные
3. Дискриминатор учится правильно классифицировать их
4. Генератор учится обманывать дискриминатор
5. Процесс повторяется до сходимости

### Функция потерь:
```
min_G max_D V(D, G) = E[log D(x)] + E[log(1 - D(G(z)))]
```
Где:
- D(x) - вероятность, что x - реальные данные
- G(z) - данные, сгенерированные из шума z


## Преимущества генеративных моделей

1. **Создание нового контента** - генерация изображений, текста, музыки
2. **Расширение датасетов** - синтез дополнительных обучающих примеров
3. **Аугментация данных** - увеличение разнообразия обучающих данных
4. **Аномалия детекция** - выявление отклонений от нормального распределения
5. **Суперразрешение** - повышение качества изображений

## Ограничения

1. **Нестабильность обучения** - особенно у GAN
2. **Проблема коллапса режимов** - модель может генерировать ограниченное множество примеров
3. **Сложность оценки качества** - нет единого метрического стандарта
4. **Высокие требования к вычислительным ресурсам** - особенно для высококачественной генерации

## Пример применения

В примере `gan_example.py` показана реализация простой GAN для генерации изображений MNIST. Генератор создает изображения из случайного шума, а дискриминатор учится отличать реальные изображения от сгенерированных. Это демонстрирует основной принцип состязательного обучения.

### Архитектура генератора:

| Входной слой | Слой 1 | Слой 2 | Слой 3 | Выходной слой |
|--------------|--------|--------|--------|---------------|
| 100 (latent_dim) | Linear: 100→256 | Linear: 256→512 | Linear: 512→1024 | Linear: 1024→784 |
|              | LeakyReLU(0.2) | LeakyReLU(0.2) | LeakyReLU(0.2) | Tanh (диапазон [-1, 1]) |

**Описание этапов генератора:**
1. **Входной слой** - случайный вектор шума размерности 100
2. **Слой 1** - преобразование: 100 нейронов → 256 нейронов с LeakyReLU активацией
3. **Слой 2** - преобразование: 256 нейронов → 512 нейронов с LeakyReLU активацией
4. **Слой 3** - преобразование: 512 нейронов → 1024 нейронов с LeakyReLU активацией
5. **Выходной слой** - преобразование: 1024 нейронов → 784 нейронов (28×28 пикселей) с Tanh активацией

### Архитектура дискриминатора:

| Входной слой | Слой 1 | Слой 2 | Слой 3 | Выходной слой |
|--------------|--------|--------|--------|---------------|
| 784 (28×28 пикселей) | Linear: 784→1024 | Linear: 1024→512 | Linear: 512→256 | Linear: 256→1 |
|              | LeakyReLU(0.2) | LeakyReLU(0.2) | LeakyReLU(0.2) | Sigmoid (вероятность) |
|              | Dropout(0.3) | Dropout(0.3) | Dropout(0.3) |               |

**Описание этапов дискриминатора:**
1. **Входной слой** - изображение MNIST размером 28×28 пикселей (784 нейронов)
2. **Слой 1** - преобразование: 784 нейронов → 1024 нейронов с LeakyReLU активацией и Dropout регуляризацией
3. **Слой 2** - преобразование: 1024 нейронов → 512 нейронов с LeakyReLU активацией и Dropout регуляризацией
4. **Слой 3** - преобразование: 512 нейронов → 256 нейронов с LeakyReLU активацией и Dropout регуляризацией
5. **Выходной слой** - преобразование: 256 нейронов → 1 нейрон с Sigmoid активацией (вероятность, что изображение настоящее)

## Известные архитектуры GAN

- **DCGAN** - использует сверточные слои
- **StyleGAN** - позволяет контролировать стиль генерируемых изображений
- **CycleGAN** - для перевода из одного домена в другой (например, лето→зима)
- **Pix2Pix** - условная генерация (например, карта→фотография)

## Применения генеративных моделей

- **Искусство и дизайн** - создание новых изображений, стилей
- **Развлечения** - генерация персонажей, миров, контента
- **Медицина** - синтез медицинских изображений для обучения
- **Мода** - дизайн одежды, создание виртуальных моделей
- **Видеоигры** - процедурная генерация контента

## Будущее генеративных моделей

С развитием технологий генеративные модели становятся всё более мощными:
- **Улучшенная стабильность обучения**
- **Более реалистичная генерация**
- **Лучший контроль над процессом генерации**
- **Мультимодальные модели** - работа с несколькими типами данных одновременно