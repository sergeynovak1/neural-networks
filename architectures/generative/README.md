# Генеративные модели

## Описание

Генеративные модели - это класс машинного обучения, который учится моделировать распределение данных с целью генерации новых примеров, похожих на обучающие данные. В отличие от дискриминативных моделей, которые учатся разделять классы, генеративные модели стремятся понять, как данные "устроены".

## Основные типы генеративных моделей

### 1. Генеративно-состязательные сети (GAN)
- Состоят из двух сетей: генератора и дискриминатора
- Обучаются в процессе "состязания" друг с другом
- Генератор создает поддельные данные, дискриминатор пытается их отличить от реальных

### 2. Вариационные автоэнкодеры (VAE)
- Вероятностный подход к автоэнкодерам
- Кодируют данные в вероятностное распределение
- Могут генерировать новые данные путем сэмплирования из латентного пространства

### 3. Нормализующие потоки (Normalizing Flows)
- Преобразуют простое распределение в сложное через последовательность обратимых преобразований
- Позволяют точно вычислять плотность вероятности

### 4. Автогрессивные модели
- Моделируют совместное распределение как произведение условных распределений
- Генерируют данные поэлементно (например, PixelRNN, WaveNet)

## GAN: Архитектура и принцип работы

### Компоненты:
1. **Генератор** - создает поддельные данные из случайного шума
2. **Дискриминатор** - отличает реальные данные от поддельных

### Процесс обучения:
1. Генератор создает поддельные данные
2. Дискриминатор получает как реальные, так и поддельные данные
3. Дискриминатор учится правильно классифицировать их
4. Генератор учится обманывать дискриминатор
5. Процесс повторяется до сходимости

### Функция потерь:
```
min_G max_D V(D, G) = E[log D(x)] + E[log(1 - D(G(z)))]
```
Где:
- D(x) - вероятность, что x - реальные данные
- G(z) - данные, сгенерированные из шума z


## Преимущества генеративных моделей

1. **Создание нового контента** - генерация изображений, текста, музыки
2. **Расширение датасетов** - синтез дополнительных обучающих примеров
3. **Аугментация данных** - увеличение разнообразия обучающих данных
4. **Аномалия детекция** - выявление отклонений от нормального распределения
5. **Суперразрешение** - повышение качества изображений

## Ограничения

1. **Нестабильность обучения** - особенно у GAN. Обучение GAN представляет собой сложный баланс между двумя состязающимися сетями. Если дискриминатор становится слишком сильным, генератор не может учиться (градиенты становятся малыми). Если генератор становится слишком сильным, дискриминатор не может отличить реальные данные от сгенерированных. Это требует тщательной настройки скорости обучения, архитектуры и других гиперпараметров для обеих сетей

2. **Проблема коллапса режимов (Mode Collapse)** - генератор начинает производить только один или несколько типов примеров вместо всего многообразия обучающих данных. Например, при генерации лиц GAN может генерировать только одно и то же лицо или ограниченный набор лиц, игнорируя остальное разнообразие датасета. Это происходит, когда генератор находит "легкий путь" обмануть дискриминатора, создавая только несколько хорошо получающихся примеров, вместо изучения всего распределения данных. Решение проблемы требует специальных техник (например, unrolled GAN, progressive GAN) или использования других архитектур (например, VAE)

3. **Сложность оценки качества** - нет единого метрического стандарта. Для генеративных моделей сложно объективно измерить качество генерируемых данных. Используются различные метрики (Inception Score, FID - Fréchet Inception Distance), но они не всегда коррелируют с человеческим восприятием качества. Часто требуется визуальная инспекция для оценки результатов

4. **Высокие требования к вычислительным ресурсам** - особенно для высококачественной генерации. GAN требуют обучения двух глубоких сетей одновременно, что требует значительных вычислительных ресурсов (GPU с большим объемом памяти). Обучение современных GAN (например, StyleGAN) может занимать дни или недели даже на мощных GPU

## Пример применения

В примере `gan_example.py` показана реализация простой GAN для генерации изображений MNIST. Генератор создает изображения из случайного шума, а дискриминатор учится отличать реальные изображения от сгенерированных. Это демонстрирует основной принцип состязательного обучения.

## Разбор примера из gan_example.py

Разберем, как работает код на примере генерации цифр MNIST:

### Архитектура двух сетей

**1. Генератор (строки 16-32) - создает изображения:**
```python
# Вход: случайный шум (100 чисел)
# Linear(100→256) → LeakyReLU → Linear(256→512) → LeakyReLU 
# → Linear(512→1024) → LeakyReLU → Linear(1024→784) → Tanh
# Выход: изображение 28×28 (784 пикселя) в диапазоне [-1, 1]
```

**2. Дискриминатор (строки 36-55) - различает реальные и поддельные:**
```python
# Вход: изображение 28×28 (784 пикселя)
# Linear(784→1024) → LeakyReLU → Dropout → Linear(1024→512) → LeakyReLU → Dropout
# → Linear(512→256) → LeakyReLU → Dropout → Linear(256→1) → Sigmoid
# Выход: вероятность (0-1), что изображение реальное
```

### Процесс обучения (строки 88-130)

Обучение происходит в два этапа на каждом батче:

**Этап 1: Обучение дискриминатора (строки 94-114)**
```python
# Реальные изображения: D(real_images) → должно быть близко к 1
# Поддельные изображения: D(G(noise)) → должно быть близко к 0
# Loss_D = loss_real + loss_fake
```

**Этап 2: Обучение генератора (строки 116-125)**
```python
# Генератор пытается обмануть дискриминатора:
# D(G(noise)) → должно быть близко к 1 (генератор хочет, чтобы его изображения считались реальными)
```

**Итог:** Генератор учится создавать реалистичные изображения, а дискриминатор учится их распознавать. В результате генератор становится лучше.

### Архитектура генератора:

| Входной слой | Слой 1 | Слой 2 | Слой 3 | Выходной слой |
|--------------|--------|--------|--------|---------------|
| 100 (latent_dim) | Linear: 100→256 | Linear: 256→512 | Linear: 512→1024 | Linear: 1024→784 |
|              | LeakyReLU(0.2) | LeakyReLU(0.2) | LeakyReLU(0.2) | Tanh (диапазон [-1, 1]) |

**Описание этапов генератора:**
1. **Входной слой** - случайный вектор шума размерности 100
2. **Слой 1** - преобразование: 100 нейронов → 256 нейронов с LeakyReLU активацией
3. **Слой 2** - преобразование: 256 нейронов → 512 нейронов с LeakyReLU активацией
4. **Слой 3** - преобразование: 512 нейронов → 1024 нейронов с LeakyReLU активацией
5. **Выходной слой** - преобразование: 1024 нейронов → 784 нейронов (28×28 пикселей) с Tanh активацией

### Архитектура дискриминатора:

| Входной слой | Слой 1 | Слой 2 | Слой 3 | Выходной слой |
|--------------|--------|--------|--------|---------------|
| 784 (28×28 пикселей) | Linear: 784→1024 | Linear: 1024→512 | Linear: 512→256 | Linear: 256→1 |
|              | LeakyReLU(0.2) | LeakyReLU(0.2) | LeakyReLU(0.2) | Sigmoid (вероятность) |
|              | Dropout(0.3) | Dropout(0.3) | Dropout(0.3) |               |

**Описание этапов дискриминатора:**
1. **Входной слой** - изображение MNIST размером 28×28 пикселей (784 нейронов)
2. **Слой 1** - преобразование: 784 нейронов → 1024 нейронов с LeakyReLU активацией и Dropout регуляризацией
3. **Слой 2** - преобразование: 1024 нейронов → 512 нейронов с LeakyReLU активацией и Dropout регуляризацией
4. **Слой 3** - преобразование: 512 нейронов → 256 нейронов с LeakyReLU активацией и Dropout регуляризацией
5. **Выходной слой** - преобразование: 256 нейронов → 1 нейрон с Sigmoid активацией (вероятность, что изображение настоящее)

## Известные архитектуры GAN

- **DCGAN** - использует сверточные слои
- **StyleGAN** - позволяет контролировать стиль генерируемых изображений
- **CycleGAN** - для перевода из одного домена в другой (например, лето→зима)
- **Pix2Pix** - условная генерация (например, карта→фотография)

## Применения генеративных моделей

- **Искусство и дизайн** - создание новых изображений, стилей
- **Развлечения** - генерация персонажей, миров, контента
- **Медицина** - синтез медицинских изображений для обучения
- **Мода** - дизайн одежды, создание виртуальных моделей
- **Видеоигры** - процедурная генерация контента

## Будущее генеративных моделей

С развитием технологий генеративные модели становятся всё более мощными:
- **Улучшенная стабильность обучения**
- **Более реалистичная генерация**
- **Лучший контроль над процессом генерации**
- **Мультимодальные модели** - работа с несколькими типами данных одновременно