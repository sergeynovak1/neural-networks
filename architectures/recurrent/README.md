# Рекуррентные нейронные сети (RNN)

## Описание

Рекуррентные нейронные сети (Recurrent Neural Networks, RNN) - это класс нейронных сетей, предназначенный для обработки последовательностных данных. В отличие от традиционных сетей, RNN имеют внутреннюю память, которая позволяет им учитывать информацию из предыдущих шагов при обработке текущего входа.

## Основные особенности

### 1. Циклические связи
- Выход сети на предыдущем шаге влияет на её состояние на следующем шаге
- Позволяет сети иметь "память" о предыдущих входах: **механизм памяти** работает через скрытое состояние (hidden state), которое передается от одного временного шага к следующему. На каждом шаге t сеть получает новый элемент последовательности x_t и предыдущее скрытое состояние h_{t-1}, вычисляет новое скрытое состояние h_t = f(x_t, h_{t-1}), которое содержит информацию о текущем и всех предыдущих элементах последовательности. Это скрытое состояние затем передается на следующий шаг, создавая "память" о всей истории входов
- Один и тот же набор параметров используется на каждом временном шаге (параметры разделяются во времени), что позволяет обрабатывать последовательности переменной длины

### 2. Обработка переменной длины
- Могут работать с последовательностями различной длины
- Подходят для текста, временных рядов, аудио и других последовательных данных

## Проблемы стандартных RNN

### 1. Исчезающий градиент (Vanishing Gradient)
- При обратном распространении ошибки через время градиенты становятся экспоненциально малыми при прохождении через много временных шагов. Это происходит из-за того, что градиент на шаге t вычисляется через произведение производных активационных функций (обычно tanh или sigmoid) на всех последующих шагах. Поскольку производные этих функций в насыщенных областях близки к нулю, их произведение быстро стремится к нулю, делая градиент практически неразличимым от нуля
- Сеть не может учиться долгосрочным зависимостям - информация о ранних элементах последовательности "забывается" из-за уменьшения градиента до нуля, что практически останавливает обновление весов на ранних временных шагах
- Затрудняет обучение глубоких рекуррентных сетей и последовательностей большой длины (более 10-20 шагов)
### 2. Взрывающийся градиент (Exploding Gradient)
- Градиенты становятся чрезмерно большими
- Приводит к нестабильности обучения
- Требует специальных методов регуляризации

## Улучшенные архитектуры

### LSTM (Long Short-Term Memory)
- Использует механизмы "ворот" для контроля потока информации
- Может эффективно учиться долгосрочным зависимостям
- Содержит ячейку памяти и три типа ворот: входные, забывающие и выходные

### GRU (Gated Recurrent Unit)
- Упрощенная версия LSTM
- Использует два типа ворот: сброса и обновления
- Часто показывает сопоставимую производительность с LSTM при меньшем количестве параметров

## Пример применения

В примере `rnn_example.py` показана реализация простой RNN для анализа тональности текста. Сеть использует слой эмбеддинга для преобразования слов в векторы, затем RNN слой для обработки последовательности, и полносвязный слой с сигмоидальной активацией для бинарной классификации.

## Разбор примера из rnn_example.py

Разберем, как работает код на примере анализа тональности текста:

### Архитектура модели (строки 13-43)

Класс `SimpleRNN` имеет три компонента:

**1. Embedding слой (строка 18)**: преобразует индексы слов в плотные векторы
```python
# Вход: [2, 5, 1, 8, ...] - индексы слов из словаря
# Embedding(vocab_size→100): каждое слово → вектор размерности 100
# Выход: (batch, seq_len, 100) - последовательность векторов
```

**2. RNN слой (строка 21)**: обрабатывает последовательность с сохранением памяти
```python
# RNN(100→128): на каждом шаге берет вектор слова + предыдущее hidden state
# Выход: hidden state размерности 128, содержащий информацию о всей последовательности
```

**3. Полносвязный слой (строки 24, 42-43)**: классификация
```python
# Linear(128→1) → Sigmoid: преобразует hidden state в вероятность (0-1)
```

### Процесс обработки текста

Для текста "это отличный фильм":
1. **Токенизация** (строки 60-67): текст → индексы [2, 5, 1, 8, ...]
2. **Embedding** (строка 31): каждый индекс → вектор [0.2, -0.5, 0.8, ...]
3. **RNN обработка** (строка 34): векторы обрабатываются последовательно, каждый шаг обновляет hidden state
4. **Последнее hidden state** (строка 39): содержит информацию обо всей фразе
5. **Классификация** (строки 42-43): преобразуется в вероятность (0.95 → "положительная")

### Обучение (строки 84-135)

- Используется BCELoss для бинарной классификации
- На входе: тексты вида "это отличный фильм" (метка: 1)
- Сеть учится понимать контекст: слово "отличный" в сочетании с другими словами → положительная тональность

### Схематичное представление архитектуры:

| Входной текст | Слой эмбеддинга | RNN слой | Выходной слой |
|---------------|-----------------|----------|---------------|
| Последовательность слов (индексов) | Embedding: vocab_size→embedding_dim | RNN: embedding_dim→hidden_dim | Linear: hidden_dim→output_dim |
| Максимальная длина: 50 | embedding_dim: 100 | hidden_dim: 128, n_layers: 1 | output_dim: 1 (бинарная классификация) |
| Batch first: True |                 | batch_first: True | Sigmoid activation |

**Описание этапов:**
1. **Входной текст** - последовательность слов, преобразованная в индексы из словаря, максимальная длина 50 слов
2. **Слой эмбеддинга** - преобразует индексы слов в плотные векторные представления размерности 100
3. **RNN слой** - обрабатывает последовательность векторов, передает информацию от предыдущих шагов к следующим, скрытое состояние имеет размерность 128
4. **Выходной слой** - использует последнее скрытое состояние для бинарной классификации с сигмоидальной активацией

## Когда использовать RNN?

RNN идеально подходят для задач:
- Анализ тональности текста
- Машинный перевод
- Генерация текста
- Распознавание речи
- Прогнозирование временных рядов
- Обработка ДНК последовательностей

## Современные альтернативы

Хотя RNN были доминирующими для последовательностных задач, в последние годы их часто заменяют:
- **Трансформеры** - используют механизм внимания вместо рекуррентных связей
- **Сверточные сети** - могут эффективно обрабатывать последовательности с помощью 1D сверток