# Рекуррентные нейронные сети (RNN)

## Описание

Рекуррентные нейронные сети (Recurrent Neural Networks, RNN) - это класс нейронных сетей, предназначенный для обработки последовательностных данных. В отличие от традиционных сетей, RNN имеют внутреннюю память, которая позволяет им учитывать информацию из предыдущих шагов при обработке текущего входа.

## Основные особенности

### 1. Циклические связи
- Выход сети на предыдущем шаге влияет на её состояние на следующем шаге
- Позволяет сети иметь "память" о предыдущих входах
- Один и тот же набор параметров используется на каждом временном шаге

### 2. Обработка переменной длины
- Могут работать с последовательностями различной длины
- Подходят для текста, временных рядов, аудио и других последовательных данных

## Проблемы стандартных RNN

### 1. Исчезающий градиент (Vanishing Gradient)
- При обратном распространении ошибка становится экспоненциально малой
- Сеть не может учиться долгосрочным зависимостям
- Затрудняет обучение глубоких рекуррентных сетей
### 2. Взрывающийся градиент (Exploding Gradient)
- Градиенты становятся чрезмерно большими
- Приводит к нестабильности обучения
- Требует специальных методов регуляризации

## Улучшенные архитектуры

### LSTM (Long Short-Term Memory)
- Использует механизмы "ворот" для контроля потока информации
- Может эффективно учиться долгосрочным зависимостям
- Содержит ячейку памяти и три типа ворот: входные, забывающие и выходные

### GRU (Gated Recurrent Unit)
- Упрощенная версия LSTM
- Использует два типа ворот: сброса и обновления
- Часто показывает сопоставимую производительность с LSTM при меньшем количестве параметров

## Пример применения

В примере `rnn_example.py` показана реализация простой RNN для анализа тональности текста. Сеть использует слой эмбеддинга для преобразования слов в векторы, затем RNN слой для обработки последовательности, и полносвязный слой с сигмоидальной активацией для бинарной классификации.

### Схематичное представление архитектуры:

| Входной текст | Слой эмбеддинга | RNN слой | Выходной слой |
|---------------|-----------------|----------|---------------|
| Последовательность слов (индексов) | Embedding: vocab_size→embedding_dim | RNN: embedding_dim→hidden_dim | Linear: hidden_dim→output_dim |
| Максимальная длина: 50 | embedding_dim: 100 | hidden_dim: 128, n_layers: 1 | output_dim: 1 (бинарная классификация) |
| Batch first: True |                 | batch_first: True | Sigmoid activation |

**Описание этапов:**
1. **Входной текст** - последовательность слов, преобразованная в индексы из словаря, максимальная длина 50 слов
2. **Слой эмбеддинга** - преобразует индексы слов в плотные векторные представления размерности 100
3. **RNN слой** - обрабатывает последовательность векторов, передает информацию от предыдущих шагов к следующим, скрытое состояние имеет размерность 128
4. **Выходной слой** - использует последнее скрытое состояние для бинарной классификации с сигмоидальной активацией

## Когда использовать RNN?

RNN идеально подходят для задач:
- Анализ тональности текста
- Машинный перевод
- Генерация текста
- Распознавание речи
- Прогнозирование временных рядов
- Обработка ДНК последовательностей

## Современные альтернативы

Хотя RNN были доминирующими для последовательностных задач, в последние годы их часто заменяют:
- **Трансформеры** - используют механизм внимания вместо рекуррентных связей
- **Сверточные сети** - могут эффективно обрабатывать последовательности с помощью 1D сверток