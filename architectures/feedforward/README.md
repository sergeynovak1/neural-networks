# Фидфорвардные сети (Feedforward Networks)

## Описание

Фидфорвардные сети (Feedforward Networks) - это самый простой тип нейронных сетей, где информация движется только в одном направлении - от входного слоя через скрытые слои к выходному слою, без циклов или обратных связей. Это основа для всех других типов нейронных сетей.

## История

Фидфорвардные сети имеют долгую историю:
- **1943** - Модель McCulloch-Pitts (первые математические нейроны)
- **1958** - Перцептрон Розенблатта
- **1986** - Алгоритм обратного распространения ошибки (backpropagation)
- **2006** - Глубокое обучение (Deep Learning)

## Архитектура

### Основные компоненты:
1. **Входной слой** - получает данные
2. **Скрытые слои** - выполняют вычисления и извлекают признаки
3. **Выходной слой** - выдает результат

### Типы фидфорвардных сетей:
- **Перцептрон** - один нейрон
- **Многослойный перцептрон (MLP)** - несколько скрытых слоев
- **Радиально-базисные функциональные сети** - используют радиальные базисные функции

## Преимущества

1. **Простота** - легко понять и реализовать, минимальная архитектура без сложных компонентов

2. **Универсальность** - могут аппроксимировать любую непрерывную функцию согласно теореме об универсальной аппроксимации (Universal Approximation Theorem), которая утверждает, что нейронная сеть с одним скрытым слоем и достаточным количеством нейронов может приблизить любую непрерывную функцию с любой желаемой точностью

3. **Параллелизм** - все нейроны в слое могут вычисляться параллельно, так как выходы предыдущего слоя полностью известны до начала вычисления текущего слоя. Это позволяет эффективно использовать GPU для ускорения обучения

4. **Стабильность** - нет проблем с циклами и обратными связями, которые могут вызывать нестабильность в рекуррентных сетях. Архитектура проста для анализа и отладки

## Ограничения

1. **Не подходят для последовательностных данных** - не имеют внутренней памяти, каждый вход обрабатывается независимо

2. **Проблемы с глубокими сетями** - **исчезающий градиент (vanishing gradient problem)**: при обратном распространении ошибки через много слоев градиенты становятся экспоненциально маленькими (близкими к нулю) в ранних слоях. Это происходит из-за того, что градиент вычисляется как произведение производных активационных функций на каждом слое. Если производные малы (например, для сигмоиды или tanh в насыщенных областях), их произведение быстро стремится к нулю, практически останавливая обучение ранних слоев

3. **Не эффективны для структурированных данных** - плохо работают с изображениями, звуком без модификаций. Например, для изображений MLP требует "вытягивания" 2D/3D структуры в 1D вектор, что полностью разрушает пространственные связи между соседними пикселями. Для изображения 224×224 это означало бы более 150,000 входных нейронов и миллионы параметров в первом слое, что неэффективно и не сохраняет важные пространственные паттерны

4. **Черный ящик** - сложно интерпретировать внутренние представления и понять, какие признаки использует сеть для принятия решений

## Пример применения

В примере `feedforward_example.py` показана реализация многослойного перцептрона (MLP) для задачи многоклассовой классификации. Сеть состоит из входного слоя, двух скрытых слоев с ReLU активацией и Dropout регуляризацией, и выходного слоя с Softmax активацией для классификации.

## Разбор примера из feedforward_example.py

Давайте разберем, как работает код на практике:

### Архитектура модели (строки 15-38)

Класс `MLP` создает сеть со следующими параметрами:
- **Входной размер**: 20 признаков (из синтетических данных)
- **Скрытые слои**: `[64, 32]` - два слоя с 64 и 32 нейронами
- **Выходной слой**: 3 класса (задача трехклассовой классификации)

**Процесс forward (строка 37-38):**
```python
# Вход: тензор формы (batch_size, 20)
x → Linear(20→64) → ReLU → Dropout(0.2) →  # Скрытый слой 1
  → Linear(64→32) → ReLU → Dropout(0.2) →  # Скрытый слой 2  
  → Linear(32→3) → Softmax                  # Выходной слой
# Выход: тензор формы (batch_size, 3) с вероятностями классов
```

### Обучение (строки 64-114)

1. **Генерация данных** (строки 41-61): создается 1000 образцов с 20 признаками, разделяется на train/test (80/20)
2. **Цикл обучения** (строки 95-110):
   - Батчи по 32 образца
   - Forward pass: данные проходят через сеть
   - Вычисляется loss (CrossEntropyLoss между предсказаниями и метками)
   - Backward pass: вычисляются градиенты
   - Обновление весов через Adam оптимизатор
3. **Результат**: сеть учится классифицировать данные на 3 класса с точностью ~95%+

## Когда использовать фидфорвардные сети?

Фидфорвардные сети подходят для задач:
- Классификация и регрессия с табличными данными
- Приближение математических функций
- Задачи с фиксированным размером входных данных
- В качестве компонента в более сложных архитектурах

## Известные архитектуры

- **Перцептрон Розенблатта** - первый алгоритм обучения нейронной сети
- **LeNet-5** - ранняя CNN, но основана на фидфорвардной архитектуре
- **MLP** - стандартный многослойный перцептрон

## Схематичное представление архитектуры MLP

На основе примера `feedforward_example.py`, ниже представлена упрощенная схема многослойного перцептрона (MLP) в виде таблицы:

| Входной слой | Скрытый слой 1 | Скрытый слой 2 | Выходной слой |
|--------------|----------------|----------------|---------------|
| 20 нейронов  | 64 нейрона     | 32 нейрона     | 3 нейрона     |
| Входные признаки | ReLU        | ReLU           | Softmax       |
|              | Dropout        | Dropout        | 3 класса      |

Более подробная архитектура сети:
- Входной слой: 20 нейронов (соответствует количеству признаков входных данных)
- Скрытый слой 1: 64 нейрона с ReLU активацией и Dropout (20%)
- Скрытый слой 2: 32 нейрона с ReLU активацией и Dropout (20%)
- Выходной слой: 3 нейрона с Softmax активацией (соответствует количеству классов)

## Современное применение

Хотя фидфорвардные сети не являются передовым решением для многих задач, они остаются важными:
- В составе более сложных архитектур (например, в конце CNN)
- Для задач с табличными данными
- Как базовые модели для сравнения