# Обучение с учителем (Supervised Learning)

## Описание

Обучение с учителем — это тип машинного обучения, при котором модель обучается на размеченных данных, то есть на данных, где для каждого входного примера имеется правильный ответ (метка).

### Математическая формулировка

**Формальное определение**: 

Дано обучающее множество D = {(x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ)}, где:
- xᵢ ∈ X — входной вектор признаков (features)
- yᵢ ∈ Y — целевое значение (label)
- n — количество обучающих примеров

**Задача**: Найти функцию f: X → Y из гипотезного пространства H, которая минимизирует ожидаемую ошибку:

f* = argmin_{f∈H} E_{(x,y)~P}[L(f(x), y)]

где:
- P — распределение данных (x, y) ~ P
- L: Y × Y → ℝ — функция потерь (loss function)
- H — гипотезное пространство (множество возможных функций)

**Эмпирический риск**: На практике минимизируем эмпирический риск:

R_emp(f) = (1/n) Σᵢ₌₁ⁿ L(f(xᵢ), yᵢ)

**Функции потерь**:
- **Классификация**: 
  - Cross-entropy: L(y, ŷ) = -Σᵢ yᵢ log(ŷᵢ)
  - Hinge loss: L(y, ŷ) = max(0, 1 - y·ŷ)
- **Регрессия**:
  - MSE: L(y, ŷ) = (y - ŷ)²
  - MAE: L(y, ŷ) = |y - ŷ|

## Содержание директории

- [README.md](README.md) - Это описание
- [supervised_example.py](supervised_example.py) - Практический пример с линейной регрессией

## Основные характеристики

- **Наличие меток**: Да, для каждого входа есть правильный ответ
- **Цель**: Предсказание значений для новых, невиданных данных
- **Обратная связь**: Явная, через правильные ответы
- **Оценка качества**: Возможна объективная оценка через сравнение предсказаний с истинными значениями

## Типы задач

### Классификация
- Предсказание дискретной метки (категории)
- Примеры: распознавание спама, диагностика заболеваний

### Регрессия
- Предсказание непрерывного значения
- Примеры: прогноз цен, предсказание температуры

## Популярные алгоритмы

### Линейная регрессия

**Математическая формулировка**:
f(x) = wᵀx + b, где w — веса, b — смещение

**Целевая функция**: min_{w,b} Σᵢ (yᵢ - wᵀxᵢ - b)²

**Когда использовать**: 
- Линейная зависимость между признаками и целевой переменной
- Интерпретируемость важна
- Мало данных

**Гиперпараметры**: 
- Регуляризация (α в Ridge, λ в Lasso)
- Learning rate (для градиентного спуска)

### Логистическая регрессия

**Математическая формулировка**:
P(y=1|x) = σ(wᵢx + b) = 1/(1 + e^{-(wᵢx + b)})

где σ — сигмоидная функция

**Когда использовать**: 
- Бинарная классификация
- Нужна вероятность принадлежности классу
- Линейно разделимые данные

### Деревья решений

**Принцип работы**: 
- Рекурсивное разбиение пространства признаков
- Выбор разбиения: max I(Y; Xⱼ), где I — информационный выигрыш (information gain)

**Критерии разбиения**:
- Gini impurity: G = 1 - Σᵢ pᵢ²
- Entropy: H = -Σᵢ pᵢ log₂(pᵢ)
- Information gain: IG = H(parent) - Σᵢ (nᵢ/n) H(childᵢ)

**Гиперпараметры**:
- `max_depth`: максимальная глубина дерева (контроль overfitting)
- `min_samples_split`: минимальное количество образцов для разбиения узла
- `min_samples_leaf`: минимальное количество образцов в листе

**Когда использовать**:
- Нужна интерпретируемость
- Нелинейные зависимости
- Смешанные типы данных

### Случайный лес (Random Forest)

**Принцип работы**: 
- Ансамбль деревьев решений
- Каждое дерево обучается на bootstrap выборке
- При разбиении узла выбирается случайное подмножество признаков

**Гиперпараметры**:
- `n_estimators`: количество деревьев (обычно 100-500)
- `max_features`: количество признаков для рассмотрения при разбиении (√n_features для классификации, n_features/3 для регрессии)
- `max_depth`: глубина деревьев

**Когда использовать**:
- Высокая размерность данных
- Нужна устойчивость к переобучению
- Хорошая производительность "из коробки"

### Метод опорных векторов (SVM)

**Математическая формулировка**:

Для линейно разделимых данных:
min_{w,b} (1/2)||w||² при условии yᵢ(wᵢxᵢ + b) ≥ 1

Для нелинейно разделимых (soft margin):
min_{w,b,ξ} (1/2)||w||² + CΣᵢ ξᵢ при условии yᵢ(wᵢxᵢ + b) ≥ 1 - ξᵢ

**Kernel trick**: Использование ядерных функций для работы в пространстве высокой размерности:
- Линейное: K(x, x') = xᵀx'
- Полиномиальное: K(x, x') = (γxᵀx' + r)ᵈ
- RBF: K(x, x') = exp(-γ||x - x'||²)

**Гиперпараметры**:
- `C`: параметр регуляризации (больше C → меньше margin, меньше ошибок на обучающей выборке)
- `kernel`: тип ядра ('linear', 'poly', 'rbf', 'sigmoid')
- `gamma`: параметр для RBF/poly ядер (больше gamma → более сложная граница решения)

**Когда использовать**:
- Высокая размерность данных
- Эффективная память (использует только опорные векторы)
- Нелинейные зависимости (с kernel trick)

### Наивный байесовский классификатор

**Математическая формулировка**:

Использует теорему Байеса:
P(y|x) = P(x|y)P(y) / P(x)

**Наивное предположение**: Признаки условно независимы при данном классе:
P(x|y) = Πᵢ P(xᵢ|y)

**Гиперпараметры**:
- `alpha`: параметр сглаживания Лапласа (для избежания нулевых вероятностей)
- Распределение признаков: Gaussian, Multinomial, Bernoulli

**Когда использовать**:
- Текстовые данные (bag of words)
- Много признаков
- Быстрая классификация
- Интерпретируемость

### Нейронные сети

**Математическая формулировка**:

Многослойный перцептрон:
- Входной слой: a⁽⁰⁾ = x
- Скрытые слои: a⁽ˡ⁾ = σ(W⁽ˡ⁾a⁽ˡ⁻¹⁾ + b⁽ˡ⁾)
- Выходной слой: ŷ = a⁽ᴸ⁾

**Обратное распространение ошибки (Backpropagation)**:
- Вычисление градиентов через цепное правило
- Обновление весов: w ← w - α∇w L

**Гиперпараметры**:
- `hidden_layers`: количество и размер скрытых слоев
- `learning_rate`: скорость обучения (обычно 0.001-0.1)
- `batch_size`: размер батча (32, 64, 128, 256)
- `epochs`: количество эпох обучения
- `activation`: функция активации ('relu', 'sigmoid', 'tanh')
- `optimizer`: алгоритм оптимизации ('adam', 'sgd', 'rmsprop')
- `dropout`: вероятность dropout для регуляризации (0.2-0.5)

**Когда использовать**:
- Сложные нелинейные зависимости
- Большие объемы данных
- Глубокое обучение для сложных задач (компьютерное зрение, NLP)

### K-ближайших соседей (KNN)

**Принцип работы**: 
- Классификация: большинство голосов среди k ближайших соседей
- Регрессия: среднее значение k ближайших соседей

**Расстояния**:
- Евклидово: d(x, x') = √Σᵢ(xᵢ - x'ᵢ)²
- Манхэттенское: d(x, x') = Σᵢ|xᵢ - x'ᵢ|
- Косинусное: d(x, x') = 1 - (xᵀx')/(||x||·||x'||)

**Гиперпараметры**:
- `k`: количество соседей (обычно нечетное, 3-15)
- `weights`: веса соседей ('uniform', 'distance')
- `metric`: метрика расстояния

**Когда использовать**:
- Небольшие датасеты
- Нужна локальная интерпретация
- Нелинейные границы решения
- Ленивое обучение (lazy learning) — модель не обучается заранее

## Когда использовать

Используйте обучение с учителем, когда:
- У вас есть размеченные данные
- Вам нужно предсказать значения или категории
- Вы можете четко сформулировать задачу предсказания

## Выбор алгоритма: практическое руководство

### Чеклист выбора алгоритма

1. **Размер данных**:
   - < 10K примеров: SVM, KNN, деревья решений
   - 10K-1M: случайный лес, градиентный бустинг, нейронные сети
   - > 1M: нейронные сети, линейные модели с SGD

2. **Интерпретируемость**:
   - Нужна: линейная/логистическая регрессия, деревья решений
   - Не важна: нейронные сети, случайный лес

3. **Тип задачи**:
   - Линейные зависимости: линейная регрессия, логистическая регрессия
   - Нелинейные: деревья, случайный лес, SVM с ядрами, нейронные сети
   - Текстовые данные: наивный Байес, нейронные сети

4. **Время обучения**:
   - Быстро: линейные модели, KNN (ленивое обучение)
   - Средне: деревья, случайный лес
   - Долго: нейронные сети, SVM на больших данных

## Типичные проблемы и решения

### 1. Переобучение (Overfitting)

**Признаки**:
- Высокая точность на обучающей выборке, низкая на тестовой
- Большая разница между train и validation loss

**Решения**:
- **Регуляризация**: L1 (Lasso), L2 (Ridge), Elastic Net
- **Dropout**: для нейронных сетей (отключение случайных нейронов)
- **Early stopping**: остановка обучения при росте validation loss
- **Упрощение модели**: уменьшение глубины деревьев, количества параметров
- **Увеличение данных**: data augmentation, сбор больше данных
- **Cross-validation**: использование k-fold для оценки обобщающей способности

### 2. Недообучение (Underfitting)

**Признаки**:
- Низкая точность на обучающей и тестовой выборках
- Модель слишком простая

**Решения**:
- Увеличение сложности модели (больше слоев, больше деревьев)
- Уменьшение регуляризации
- Увеличение количества признаков
- Улучшение feature engineering

### 3. Несбалансированные классы

**Проблема**: Один класс значительно превосходит другой по количеству примеров

**Решения**:
- **Метрики**: использовать precision, recall, F1-score вместо accuracy
- **Resampling**: 
  - Oversampling меньшего класса (SMOTE)
  - Undersampling большего класса
- **Взвешивание классов**: class_weight='balanced' в sklearn
- **Ансамбли**: комбинация моделей, обученных на сбалансированных выборках

### 4. Пропущенные значения (Missing Values)

**Стратегии**:
- **Удаление**: если пропусков мало (< 5%)
- **Заполнение**: средним/медианой (для числовых), модой (для категориальных)
- **Предсказание**: использование модели для предсказания пропущенных значений
- **Индикаторы**: создание бинарного признака "значение пропущено"

### 5. Выбросы (Outliers)

**Обнаружение**:
- IQR метод: Q1 - 1.5×IQR, Q3 + 1.5×IQR
- Z-score: |z| > 3
- Isolation Forest

**Обработка**:
- Удаление (если это ошибки)
- Трансформация (логарифмирование, clipping)
- Использование устойчивых алгоритмов (случайный лес, SVM)

### 6. Масштабирование признаков

**Проблема**: Признаки в разных масштабах могут влиять на обучение

**Методы**:
- **Standardization**: (x - μ) / σ (среднее 0, дисперсия 1)
- **Normalization**: (x - min) / (max - min) (диапазон [0, 1])
- **Robust scaling**: использование медианы и IQR (устойчиво к выбросам)

**Когда нужно**:
- Обязательно: SVM, нейронные сети, KNN, методы на основе расстояний
- Не обязательно: деревья решений, случайный лес (но может помочь)

## Метрики оценки качества

### Классификация

**Бинарная классификация**:
- **Accuracy**: (TP + TN) / (TP + TN + FP + FN)
- **Precision**: TP / (TP + FP) — точность положительных предсказаний
- **Recall (Sensitivity)**: TP / (TP + FN) — покрытие положительного класса
- **F1-score**: 2 × (Precision × Recall) / (Precision + Recall) — гармоническое среднее
- **ROC-AUC**: площадь под кривой ROC (чувствительность vs специфичность)

**Многоклассовая классификация**:
- **Macro-averaging**: среднее метрик по классам
- **Micro-averaging**: агрегация всех TP, FP, TN, FN
- **Weighted-averaging**: среднее с весами по количеству примеров в классе

### Регрессия

- **MSE (Mean Squared Error)**: (1/n) Σ(yᵢ - ŷᵢ)²
- **RMSE (Root MSE)**: √MSE — в тех же единицах, что и целевая переменная
- **MAE (Mean Absolute Error)**: (1/n) Σ|yᵢ - ŷᵢ|
- **R² (Coefficient of Determination)**: 1 - (SS_res / SS_tot) — доля объясненной дисперсии
- **MAPE (Mean Absolute Percentage Error)**: (100/n) Σ|(yᵢ - ŷᵢ)/yᵢ|

## Рекомендации по валидации

1. **Train-Validation-Test split**:
   - Train (60-70%): обучение модели
   - Validation (15-20%): подбор гиперпараметров
   - Test (15-20%): финальная оценка

2. **K-Fold Cross-Validation**:
   - Разбиение данных на k фолдов
   - Обучение на k-1 фолдах, тестирование на 1
   - Повтор k раз, усреднение результатов
   - Рекомендуется k=5 или k=10

3. **Stratified K-Fold**:
   - Для классификации: сохранение пропорций классов в каждом фолде

4. **Time Series Split**:
   - Для временных рядов: обучение на прошлом, тестирование на будущем

## Практический пример

Запустите [supervised_example.py](supervised_example.py), чтобы увидеть пример линейной регрессии для предсказания цены дома на основе его площади.