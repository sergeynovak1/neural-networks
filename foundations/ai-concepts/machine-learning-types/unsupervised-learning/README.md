# Обучение без учителя (Unsupervised Learning)

## Описание

Обучение без учителя — это тип машинного обучения, при котором модель работает с данными без предоставления правильных ответов. Цель состоит в том, чтобы найти скрытые закономерности, структуры или зависимости в данных.

### Математическая формулировка

**Формальное определение**: 

Дано множество данных D = {x₁, x₂, ..., xₙ}, где xᵢ ∈ X — входной вектор признаков без меток.

**Задачи обучения без учителя**:

1. **Кластеризация**: Найти разбиение данных на k групп
   - Найти C = {C₁, C₂, ..., Cₖ}, минимизирующее внутрикластерную дисперсию
   - min Σᵢ₌₁ᵏ Σ_{x∈Cᵢ} d(x, μᵢ)², где μᵢ — центр кластера, d — метрика расстояния

2. **Снижение размерности**: Найти проекцию f: ℝᵈ → ℝᵏ (k < d)
   - Сохранить максимальную информацию: max I(X; f(X))
   - Минимизировать ошибку реконструкции: min ||X - f⁻¹(f(X))||²

3. **Обнаружение аномалий**: Найти точки, не соответствующие основной структуре
   - P(x) < θ, где θ — порог аномальности
   - Или: d(x, μ) > k·σ, где μ — центр, σ — стандартное отклонение, k — множитель

**Скрытые закономерности** могут включать:
- **Кластеры**: группы похожих объектов
- **Корреляции**: линейные или нелинейные зависимости между признаками
- **Латентные факторы**: скрытые переменные, объясняющие наблюдаемые данные
- **Многообразие (Manifold)**: низкоразмерная структура в высокоразмерном пространстве

## Содержание директории

- [README.md](README.md) - Это описание
- [unsupervised_example.py](unsupervised_example.py) - Практический пример с кластеризацией и PCA

## Основные характеристики

- **Наличие меток**: Нет, только входные данные
- **Цель**: Поиск скрытых структур и закономерностей
- **Обратная связь**: Отсутствует
- **Оценка качества**: Затруднена, требует экспертного анализа

## Типы задач

### Кластеризация
- Группировка похожих объектов
- Примеры: сегментация клиентов, группировка документов

### Снижение размерности
- Упрощение данных с сохранением важной информации
- Примеры: визуализация, удаление шума

### Обнаружение аномалий
- Поиск выбросов и необычных паттернов
- Примеры: обнаружение мошенничества, диагностика неисправностей

## Популярные алгоритмы

### K-средних (K-means)

**Математическая формулировка**:

Целевая функция (within-cluster sum of squares):
min Σᵢ₌₁ᵏ Σ_{x∈Cᵢ} ||x - μᵢ||²

где:
- k — количество кластеров (задается заранее)
- μᵢ — центр кластера i
- Cᵢ — множество точек в кластере i

**Алгоритм**:
1. Инициализация: случайный выбор k центроидов
2. Назначение: каждая точка назначается ближайшему центроиду
3. Обновление: центроиды пересчитываются как среднее точек в кластере
4. Повторение шагов 2-3 до сходимости

**Гиперпараметры**:
- `n_clusters` (k): количество кластеров
  - Методы выбора: Elbow method, Silhouette analysis, Gap statistic
- `init`: метод инициализации ('k-means++', 'random')
- `n_init`: количество запусков с разными инициализациями (рекомендуется 10)
- `max_iter`: максимальное количество итераций

**Ограничения**:
- Требует знания количества кластеров
- Предполагает сферические кластеры одинакового размера
- Чувствителен к выбросам
- Локальные минимумы (зависит от инициализации)

**Когда использовать**:
- Известно количество кластеров
- Кластеры сферические и примерно одинакового размера
- Большие датасеты (эффективный алгоритм)

### Иерархическая кластеризация

**Принцип работы**:
- **Агломеративная (Agglomerative)**: начинаем с n кластеров (каждая точка — кластер), объединяем ближайшие
- **Дивизивная (Divisive)**: начинаем с одного кластера, рекурсивно разделяем

**Метрики расстояния между кластерами**:
- **Single linkage**: min{d(x, y) : x ∈ C₁, y ∈ C₂}
- **Complete linkage**: max{d(x, y) : x ∈ C₁, y ∈ C₂}
- **Average linkage**: (1/|C₁||C₂|) Σ_{x∈C₁} Σ_{y∈C₂} d(x, y)
- **Ward linkage**: минимизирует увеличение within-cluster variance

**Гиперпараметры**:
- `n_clusters`: количество кластеров (или `distance_threshold` для автоматического выбора)
- `linkage`: метод связи ('ward', 'complete', 'average', 'single')
- `affinity`: метрика расстояния ('euclidean', 'manhattan', 'cosine')

**Преимущества**:
- Не требует знания количества кластеров заранее
- Визуализация через дендрограмму
- Может находить кластеры произвольной формы

**Недостатки**:
- Вычислительная сложность O(n³) для агломеративного подхода
- Чувствительность к выбросам

**Когда использовать**:
- Неизвестно количество кластеров
- Нужна визуализация структуры данных (дендрограмма)
- Небольшие или средние датасеты

### Метод главных компонент (PCA)

**Математическая формулировка**:

PCA находит линейную проекцию, максимизирующую дисперсию данных:

max_{w} Var(wᵀX) = max_{w} wᵀΣw

при условии ||w|| = 1, где Σ — ковариационная матрица.

**Решение**: Собственные векторы ковариационной матрицы, соответствующие наибольшим собственным значениям.

**Алгоритм**:
1. Центрирование данных: X_centered = X - μ
2. Вычисление ковариационной матрицы: Σ = (1/n) X_centeredᵀ X_centered
3. Нахождение собственных векторов и значений: Σv = λv
4. Выбор k главных компонент (с наибольшими λ)
5. Проекция: Y = X_centered · W, где W — матрица главных компонент

**Гиперпараметры**:
- `n_components`: количество компонент (или доля объясненной дисперсии, например 0.95)
- `svd_solver`: алгоритм ('auto', 'full', 'arpack', 'randomized')

**Объясненная дисперсия**:
- Каждая компонента объясняет часть общей дисперсии
- Рекомендуется выбирать количество компонент, объясняющих ≥ 80-95% дисперсии

**Ограничения**:
- Линейное преобразование (не работает для нелинейных зависимостей)
- Предполагает, что главные направления — это направления максимальной дисперсии
- Чувствительность к масштабированию признаков

**Когда использовать**:
- Снижение размерности для визуализации
- Удаление коррелированных признаков
- Предобработка данных перед обучением модели
- Удаление шума (если шум в компонентах с малой дисперсией)

### t-SNE (t-Distributed Stochastic Neighbor Embedding)

**Принцип работы**:
- Нелинейный метод снижения размерности для визуализации
- Сохраняет локальные структуры (близкие точки остаются близкими)
- Использует распределение Стьюдента для моделирования расстояний в низкоразмерном пространстве

**Гиперпараметры**:
- `n_components`: размерность выходного пространства (обычно 2 или 3)
- `perplexity`: баланс между локальной и глобальной структурой (обычно 5-50)
- `learning_rate`: скорость обучения (обычно 10-1000)
- `n_iter`: количество итераций (обычно 1000)

**Особенности**:
- Не сохраняет глобальную структуру (далекие точки могут оказаться близко)
- Результаты зависят от инициализации
- Вычислительно дорогой (O(n²))

**Когда использовать**:
- Визуализация высокоразмерных данных
- Исследовательский анализ данных
- Понимание локальной структуры данных

### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)

**Принцип работы**:
- Кластеризация на основе плотности
- Находит кластеры произвольной формы
- Автоматически определяет количество кластеров
- Помечает выбросы как шум

**Параметры**:
- **ε (eps)**: радиус окрестности
- **min_samples**: минимальное количество точек в ε-окрестности для ядра кластера

**Типы точек**:
- **Core point**: точка с ≥ min_samples соседей в радиусе ε
- **Border point**: точка в ε-окрестности core point, но не core point
- **Noise point**: не core и не border

**Гиперпараметры**:
- `eps`: радиус окрестности (критический параметр)
  - Слишком маленький → много маленьких кластеров
  - Слишком большой → все точки в одном кластере
- `min_samples`: минимальное количество соседей (обычно 2×размерность данных)

**Преимущества**:
- Не требует знания количества кластеров
- Находит кластеры произвольной формы
- Устойчив к выбросам (помечает их как шум)

**Недостатки**:
- Проблемы с кластерами разной плотности
- Чувствительность к параметру eps
- Плохо работает с высокоразмерными данными

**Когда использовать**:
- Неизвестно количество кластеров
- Кластеры произвольной формы
- Есть выбросы, которые нужно обнаружить
- Данные с разной плотностью кластеров

### Автокодировщики (Autoencoders)

**Математическая формулировка**:

Автокодировщик состоит из двух частей:
- **Encoder**: f: ℝᵈ → ℝᵏ (k < d), сжимает данные
- **Decoder**: g: ℝᵏ → ℝᵈ, восстанавливает данные

**Целевая функция**: min ||x - g(f(x))||²

**Архитектура**:
- Входной слой: d нейронов
- Скрытые слои (encoder): уменьшение размерности
- Бottleneck (latent space): k нейронов (k << d)
- Скрытые слои (decoder): увеличение размерности
- Выходной слой: d нейронов

**Варианты**:
- **Sparse Autoencoder**: добавление регуляризации для разреженности
- **Denoising Autoencoder**: обучение на зашумленных данных
- **Variational Autoencoder (VAE)**: вероятностная модель с нормальным распределением в latent space

**Гиперпараметры**:
- Архитектура: количество слоев и нейронов
- Размерность latent space (bottleneck)
- Функция активации
- Learning rate, batch size, epochs

**Когда использовать**:
- Нелинейное снижение размерности
- Обучение представлений (representation learning)
- Предобработка данных
- Обнаружение аномалий (высокая ошибка реконструкции → аномалия)

## Когда использовать

Используйте обучение без учителя, когда:
- У вас есть данные, но нет меток
- Вы хотите понять структуру данных
- Вы занимаетесь исследовательским анализом

## Curse of Dimensionality (Проклятие размерности)

### Проблема

В пространствах высокой размерности возникают следующие проблемы:

1. **Равноудаленность точек**: Все точки становятся примерно равноудаленными друг от друга
   - Расстояние между случайными точками: d ≈ √d · σ (где d — размерность, σ — стандартное отклонение)
   - В высоких размерностях это расстояние становится примерно постоянным

2. **Пустота пространства**: Данные занимают ничтожную долю объема пространства
   - Объем гиперсферы: V(r) = (π^(d/2) / Γ(d/2+1)) · r^d
   - Большая часть объема сосредоточена в тонком слое около поверхности

3. **Неэффективность алгоритмов**: Многие алгоритмы теряют эффективность
   - Методы на основе расстояний (KNN, кластеризация) перестают работать
   - Необходимость экспоненциально больше данных для покрытия пространства

### Решения

1. **Снижение размерности**:
   - PCA: линейное снижение размерности
   - t-SNE, UMAP: нелинейное снижение для визуализации
   - Autoencoders: нелинейное снижение для обучения представлений

2. **Feature Selection (Отбор признаков)**:
   - Удаление нерелевантных признаков
   - Методы: корреляционный анализ, mutual information, recursive feature elimination

3. **Алгоритмы, устойчивые к высокой размерности**:
   - Random Forest: работает с подмножествами признаков
   - Neural Networks: могут извлекать полезные признаки автоматически

## Выбор алгоритма кластеризации

### Чеклист выбора

1. **Известно ли количество кластеров?**
   - Да → K-means, Gaussian Mixture Models
   - Нет → Иерархическая кластеризация, DBSCAN

2. **Форма кластеров**:
   - Сферические → K-means
   - Произвольная → DBSCAN, иерархическая кластеризация

3. **Размер датасета**:
   - Маленький (< 10K) → Иерархическая кластеризация, DBSCAN
   - Большой (> 10K) → K-means, Mini-batch K-means

4. **Есть ли выбросы?**
   - Да → DBSCAN (помечает как шум)
   - Нет → K-means, иерархическая кластеризация

5. **Нужна ли интерпретируемость?**
   - Да → Иерархическая кластеризация (дендрограмма)
   - Не важна → K-means, DBSCAN

## Метрики оценки качества кластеризации

### Внутренние метрики (без ground truth)

1. **Silhouette Score**:
   - s(i) = (b(i) - a(i)) / max(a(i), b(i))
   - a(i): среднее расстояние до точек в том же кластере
   - b(i): среднее расстояние до точек в ближайшем другом кластере
   - Диапазон: [-1, 1], чем выше, тем лучше

2. **Davies-Bouldin Index**:
   - DB = (1/k) Σᵢ max_{j≠i} ((σᵢ + σⱼ) / d(μᵢ, μⱼ))
   - Чем меньше, тем лучше

3. **Calinski-Harabasz Index (Variance Ratio)**:
   - CH = (SSB / (k-1)) / (SSW / (n-k))
   - SSB: between-cluster scatter, SSW: within-cluster scatter
   - Чем выше, тем лучше

### Внешние метрики (с ground truth)

1. **Adjusted Rand Index (ARI)**:
   - Сравнение с истинными метками
   - Диапазон: [-1, 1], 1 — идеальное совпадение

2. **Normalized Mutual Information (NMI)**:
   - NMI = I(C; K) / √(H(C) · H(K))
   - Диапазон: [0, 1], 1 — идеальное совпадение

### Методы выбора количества кластеров

1. **Elbow Method**:
   - Построение графика зависимости within-cluster sum of squares от k
   - Выбор k в точке "локтя" (резкое замедление уменьшения ошибки)

2. **Silhouette Analysis**:
   - Построение графика silhouette score от k
   - Выбор k с максимальным silhouette score

3. **Gap Statistic**:
   - Сравнение с ожидаемым значением для случайных данных
   - Выбор k, максимизирующего gap

## Типичные проблемы и решения

### 1. Выбор количества кластеров

**Проблема**: Неизвестно оптимальное количество кластеров

**Решения**:
- Использовать Elbow method или Silhouette analysis
- Попробовать иерархическую кластеризацию для визуализации структуры
- Использовать DBSCAN, который определяет количество автоматически
- Domain knowledge: использовать экспертные знания о данных

### 2. Масштабирование признаков

**Проблема**: Признаки в разных масштабах искажают расстояния

**Решения**:
- **Standardization**: (x - μ) / σ — обязательно для K-means, PCA
- **Normalization**: (x - min) / (max - min)
- **Robust scaling**: использование медианы и IQR (устойчиво к выбросам)

**Когда нужно**:
- Обязательно: K-means, иерархическая кластеризация, PCA
- Не обязательно: DBSCAN (но может помочь)

### 3. Выбросы

**Проблема**: Выбросы могут исказить результаты кластеризации

**Решения**:
- Использовать DBSCAN (автоматически помечает выбросы)
- Предобработка: удаление выбросов перед кластеризацией
- Использовать robust метрики расстояния (Manhattan вместо Euclidean)
- Применять методы обнаружения аномалий отдельно

### 4. Кластеры разной плотности

**Проблема**: K-means предполагает одинаковую плотность кластеров

**Решения**:
- Использовать DBSCAN или иерархическую кластеризацию
- Применять Gaussian Mixture Models (GMM) с разными ковариациями
- Использовать OPTICS (расширение DBSCAN для разных плотностей)

### 5. Высокая размерность

**Проблема**: Curse of dimensionality делает кластеризацию неэффективной

**Решения**:
- Снижение размерности перед кластеризацией (PCA, t-SNE, autoencoders)
- Использование подпространств (subspace clustering)
- Feature selection для удаления нерелевантных признаков

## Рекомендации по предобработке

1. **Обработка пропущенных значений**:
   - Удаление или заполнение (средним, медианой, модой)
   - Использование алгоритмов, устойчивых к пропускам

2. **Масштабирование**:
   - Всегда для K-means, PCA, иерархической кластеризации
   - Проверка влияния на DBSCAN

3. **Удаление выбросов**:
   - Если они не представляют интереса
   - Или использование алгоритмов, устойчивых к выбросам

4. **Снижение размерности**:
   - Если признаков > 50-100
   - Для визуализации и ускорения алгоритмов

## Практический пример

Запустите [unsupervised_example.py](unsupervised_example.py), чтобы увидеть примеры кластеризации клиентов и снижения размерности с помощью PCA.