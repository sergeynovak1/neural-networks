# Типы машинного обучения

## Введение

Машинное обучение — это область искусственного интеллекта, которая позволяет компьютерам учиться и принимать решения на основе данных без явного программирования. Существует три основных типа машинного обучения, каждый из которых имеет свои особенности, применения и алгоритмы.

### Математическая формулировка задачи машинного обучения

В общем виде задача машинного обучения может быть сформулирована следующим образом:

**Дано**: 
- Множество данных D = {(x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ)}, где xᵢ ∈ X — входные данные, yᵢ ∈ Y — целевые значения (могут отсутствовать)
- Гипотезное пространство H — множество функций h: X → Y

**Найти**: 
- Функцию h* ∈ H, которая минимизирует ожидаемую ошибку на новых данных

**Целевая функция**:
- Для обучения с учителем: h* = argmin_{h∈H} E[L(h(x), y)], где L — функция потерь
- Для обучения без учителя: h* = argmin_{h∈H} E[L(x, h(x))], где цель — найти структуру в данных
- Для обучения с подкреплением: π* = argmax_{π} E[∑ᵢ γᵢ rᵢ], где π — политика, r — награда, γ — коэффициент дисконтирования

## Исторический контекст развития типов машинного обучения

Каждый из трех основных типов машинного обучения развивался в ответ на конкретные технологические и научные вызовы своего времени.

### Обучение с учителем (Supervised Learning)

**Когда появилось**: 1957 год с созданием персептрона Розенблаттом

**Конкретные события и причины появления**:
- **1957**: Фрэнк Розенблатт создал персептрон - первую модель нейронной сети, способную обучаться на примерах с правильными ответами. Именно эта модель стала первым воплощением обучения с учителем.
- **1960-е**: Исследования в области распознавания образов показали, что для решения практических задач (распознавание символов, классификация изображений) необходимо иметь обучающие примеры с известными ответами.
- **1970-е**: Появление первых алгоритмов линейной регрессии и классификации, которые требовали размеченных данных для обучения.

**Почему именно этот подход**: Персептрон показал, что можно автоматически настраивать параметры системы на основе сравнения предсказанных и реальных результатов - это и стало основой обучения с учителем.

### Обучение без учителя (Unsupervised Learning)

**Когда появилось**: Конец 1960-х - начало 1970-х годов

**Конкретные события и причины появления**:
- **1967**: Разработка алгоритма k-ближайших соседей (k-nearest neighbors) - одного из первых алгоритмов, который мог работать с данными без явных меток.
- **1970-е**: Развитие статистических методов кластеризации (k-means был предложен в 1967 году) и факторного анализа показало, что можно находить скрытые структуры в данных без знания "правильных" ответов.
- **1980-е**: Интерес к самоорганизующимся картам Кохонена, которые могли находить закономерности в данных без внешнего контроля.

**Почему именно этот подход**: Когда исследователи столкнулись с высокой стоимостью разметки данных вручную, они начали искать способы извлекать полезную информацию из "сырых" данных, что привело к появлению обучения без учителя.

### Обучение с подкреплением (Reinforcement Learning)

**Когда появилось**: 1972 год (формулировка проблемы), активное развитие с 1988 года

**Конкретные события и причины появления**:
- **1972**: Ричард Саттон (Richard Sutton) и Эндрю Бартоло (Andrew Barto) начали работу над алгоритмами обучения с подкреплением, вдохновленные исследованиями в области поведенческой психологии (закон эффекта Торндайка).
- **1988**: Публикация статьи Уоткинса о Q-learning - первого алгоритма, который мог обучать агентов принимать последовательные решения.
- **1992**: Создание программы TD-Gammon для игры в нарды, которая показала силу обучения с подкреплением.
- **1997**: IBM Deep Blue победила Гарри Каспарова в шахматах, что показало возможности подходов, основанных на оптимизации последовательных решений.

**Почему именно этот подход**: В отличие от задач классификации, где есть четкие правильные ответы, в задачах управления и игр необходимо было обучать системы принимать последовательные решения для достижения долгосрочной цели - это и стало основой обучения с подкреплением.

### Современное состояние и тренды

Сегодня все три подхода активно развиваются и интегрируются:

#### Традиционные применения:
- **Обучение с учителем** доминирует в задачах классификации и регрессии (компьютерное зрение, обработка текста)
- **Обучение без учителя** используется для предобработки данных, кластеризации и снижения размерности
- **Обучение с подкреплением** применяется в робототехнике, игровых ИИ и системах управления

#### Современные направления развития:

**Гибридные подходы:**
- **Semi-supervised Learning (Полуобучение с учителем)**: Комбинация размеченных и неразмеченных данных. Используется когда разметка дорога, но есть много неразмеченных данных. Примеры: классификация текстов с небольшим количеством размеченных примеров.

- **Self-supervised Learning (Самообучение)**: Подход, где модель создает собственные обучающие сигналы из данных. Широко используется в компьютерном зрении (например, предобучение на задаче предсказания повернутых изображений) и обработке естественного языка (BERT, GPT).

- **Transfer Learning (Перенос обучения)**: Использование знаний, полученных на одной задаче, для решения другой. Примеры: использование предобученных моделей ImageNet для медицинской диагностики, fine-tuning языковых моделей для конкретных доменов.

- **Foundation Models (Фундаментальные модели)**: Крупномасштабные модели, предобученные на огромных объемах данных и адаптируемые для широкого спектра задач. Примеры: GPT, BERT, CLIP, DALL-E.

**Интеграция подходов:**
- **Contrastive Learning**: Комбинация unsupervised и self-supervised подходов для обучения представлений без явных меток
- **Imitation Learning**: Использование supervised learning для инициализации RL-агентов
- **Meta-Learning**: Обучение алгоритмов обучения, часто комбинируя все три подхода

## Основные типы машинного обучения

### 1. [Обучение с учителем (Supervised Learning)](supervised-learning/)
Обучение на размеченных данных с известными правильными ответами.

### 2. [Обучение без учителя (Unsupervised Learning)](unsupervised-learning/)
Поиск скрытых закономерностей в данных без правильных ответов.

### 3. [Обучение с подкреплением (Reinforcement Learning)](reinforcement-learning/)
Обучение через взаимодействие со средой и получение наград/штрафов.

## Сравнительная таблица

| Характеристика | С учителем | Без учителя | С подкреплением |
|----------------|------------|-------------|-----------------|
| **Наличие меток** | Да | Нет | Частичные награды (sparse rewards) |
| **Цель** | Предсказание | Поиск структур | Максимизация наград |
| **Обратная связь** | Явная | Отсутствует | Через награды |
| **Примеры задач** | Классификация, регрессия | Кластеризация, снижение размерности | Игры, управление |
| **Типичные алгоритмы** | SVM, деревья, нейронные сети | K-means, PCA | Q-learning, Policy Gradient |
| **Оценка качества** | Точность, F1 и др. | Сложно оценить | Суммарная награда |
| **Пример из жизни** | Распознавание изображений | Рекомендации | Шахматы, видеоигры |

## Подробное сравнение

### Обучение с учителем (Supervised Learning)

**Математическая формулировка:**

Задача обучения с учителем: найти функцию f: X → Y, которая минимизирует ожидаемую ошибку:

f* = argmin_{f∈F} E_{(x,y)~P}[L(f(x), y)]

где:
- X — пространство входных данных
- Y — пространство выходных данных (меток)
- P — распределение данных
- L — функция потерь (loss function)
- F — гипотезное пространство

**Принцип работы:**
- Использует размеченные данные (вход-выход) вида D = {(x₁, y₁), ..., (xₙ, yₙ)}
- Алгоритм учится отображать входы в выходы, минимизируя функцию потерь
- Можно объективно оценить качество через метрики (accuracy, precision, recall, F1, MSE, MAE)

**Теоретические основы:**
- **PAC-обучение (Probably Approximately Correct)**: Теоретический framework, определяющий условия, при которых алгоритм может обучиться с заданной точностью и уверенностью. Формально: алгоритм PAC-обучаем, если существует полиномиальная функция m(ε, δ), такая что для любого распределения P и любых ε, δ > 0, после обучения на m примерах, с вероятностью ≥ 1-δ ошибка ≤ ε.

- **Теорема "No Free Lunch"**: Не существует универсального алгоритма, который работает лучше всех остальных на всех возможных задачах. Каждый алгоритм имеет свои предположения (inductive bias), которые определяют, на каких задачах он будет работать хорошо.

**Преимущества:**
- Хорошо работает при наличии качественных размеченных данных
- Можно объективно оценить качество модели через метрики
- Много готовых алгоритмов и инструментов
- Сильная теоретическая база и гарантии обобщения

**Недостатки:**
- Требует размеченные данные, которые могут быть дороги в получении
- Риск переобучения (overfitting) — модель запоминает обучающие данные вместо обобщения
- Модель может не обобщаться на данные, отличные от обучающей выборки (проблема domain shift)
- Зависимость от качества разметки (garbage in, garbage out)

### Обучение без учителя (Unsupervised Learning)

**Математическая формулировка:**

Задача обучения без учителя: найти структуру в данных без явных меток. Основные типы задач:

1. **Кластеризация**: Разбить данные на группы
   - Найти разбиение C = {C₁, ..., Cₖ}, минимизирующее внутрикластерное расстояние
   - min Σᵢ Σ_{x∈Cᵢ} d(x, μᵢ), где μᵢ — центр кластера, d — метрика расстояния

2. **Снижение размерности**: Найти проекцию в пространство меньшей размерности
   - Найти функцию f: ℝᵈ → ℝᵏ (k < d), сохраняющую максимальную информацию
   - max I(X; f(X)), где I — взаимная информация

3. **Обнаружение аномалий**: Найти точки, не соответствующие основной структуре
   - Найти точки x, такие что P(x) < θ, где θ — порог

**Принцип работы:**
- Работает только с входными данными без меток D = {x₁, ..., xₙ}
- Ищет скрытые закономерности и структуры через оптимизацию целевой функции (например, минимизация внутрикластерной дисперсии)
- Результаты интерпретируются человеком или используются для дальнейшей обработки

**Теоретические основы:**
- **Curse of Dimensionality (Проклятие размерности)**: В пространствах высокой размерности все точки становятся примерно равноудаленными друг от друга, что делает многие алгоритмы неэффективными. Это одна из основных причин использования снижения размерности.

- **Теорема о концентрации меры**: В высоких размерностях большая часть объема гиперсферы сосредоточена в тонком слое около поверхности, что усложняет поиск структуры.

**Преимущества:**
- Не требует размеченных данных (данные часто дешевле и доступнее)
- Может обнаруживать неожиданные закономерности, которые не были известны заранее
- Полезна для исследования данных (exploratory data analysis)
- Может работать с большими объемами данных

**Недостатки:**
- Сложно оценить качество результатов объективно (нет ground truth)
- Результаты могут быть трудно интерпретируемыми
- Требует экспертного анализа для понимания значимости паттернов
- Многие алгоритмы требуют задания числа кластеров или других гиперпараметров заранее
- Чувствительность к масштабированию признаков и выбросам

### Обучение с подкреплением (Reinforcement Learning)

**Математическая формулировка:**

RL формулируется как Марковский процесс принятия решений (Markov Decision Process, MDP):

MDP = (S, A, P, R, γ)

где:
- S — множество состояний
- A — множество действий
- P(s'|s, a) — вероятность перехода в состояние s' из состояния s при действии a
- R(s, a, s') — функция награды
- γ ∈ [0, 1] — коэффициент дисконтирования

**Цель**: Найти оптимальную политику π*: S → A, максимизирующую ожидаемую дисконтированную награду:

π* = argmax_{π} E[∑_{t=0}^∞ γᵗ rₜ | π]

**Функция ценности состояния**:
V^π(s) = E[∑_{k=0}^∞ γᵏ r_{t+k+1} | s_t = s, π]

**Функция Q-значения**:
Q^π(s, a) = E[∑_{k=0}^∞ γᵏ r_{t+k+1} | s_t = s, a_t = a, π]

**Уравнение Беллмана**:
Q*(s, a) = E[r + γ max_{a'} Q*(s', a') | s, a]

**Принцип работы:**
- Агент взаимодействует со средой в дискретные моменты времени
- В каждом состоянии s_t агент выбирает действие a_t согласно политике π
- Среда переходит в новое состояние s_{t+1} и выдает награду r_t
- Агент учится максимизировать долгосрочную дисконтированную награду G_t = ∑_{k=0}^∞ γᵏ r_{t+k+1}

**Теоретические основы:**
- **Теорема о сходимости Q-learning**: При определенных условиях (все пары состояние-действие посещаются бесконечно часто, learning rate удовлетворяет условиям Роббинса-Монро) Q-learning сходится к оптимальной Q-функции Q*.

- **Exploration-Exploitation Dilemma**: Баланс между исследованием новых действий (exploration) и использованием известных хороших действий (exploitation). Решается через ε-greedy, UCB, Thompson Sampling.

**Преимущества:**
- Может решать задачи последовательного принятия решений, где текущее действие влияет на будущие состояния
- Не требует размеченных данных в традиционном смысле (только сигнал награды)
- Подходит для задач управления и оптимизации, где есть долгосрочные цели
- Может обучаться в интерактивной среде

**Недостатки:**
- Требует много времени для обучения (много взаимодействий со средой)
- Сложно настроить функцию награды (reward shaping) — неправильная награда может привести к нежелательному поведению
- Проблемы с безопасностью во время обучения (агент может совершать опасные действия)
- Высокая вариативность результатов обучения
- Проблема sparse rewards — награды могут быть редкими, что замедляет обучение

## Когда использовать каждый тип?

### Выбирайте обучение с учителем, если:
- У вас есть размеченные данные
- Вам нужно предсказать значения или категории
- Вы можете четко сформулировать задачу предсказания

### Выбирайте обучение без учителя, если:
- У вас есть данные, но нет меток
- Вы хотите понять структуру данных
- Вы занимаетесь исследовательским анализом

### Выбирайте обучение с подкреплением, если:
- Задача предполагает последовательность решений
- Можно смоделировать среду взаимодействия
- Цель — оптимизация долгосрочного результата

## Теоретические ограничения и компромиссы

### Теорема "No Free Lunch" (NFL)

**Формулировка**: Для любого алгоритма обучения, его средняя производительность по всем возможным задачам одинакова. Нет универсального "лучшего" алгоритма.

**Практическое значение**: 
- Выбор алгоритма должен зависеть от конкретной задачи и данных
- Необходимо понимать предположения (inductive bias) каждого алгоритма
- Комбинация разных подходов часто дает лучшие результаты

### Bias-Variance Tradeoff

**Компромисс между смещением и дисперсией**:

Ожидаемая ошибка = Bias² + Variance + Irreducible Error

- **Bias (Смещение)**: Ошибка из-за слишком простой модели (underfitting)
- **Variance (Дисперсия)**: Ошибка из-за слишком сложной модели (overfitting)
- **Irreducible Error**: Неустранимая ошибка из-за шума в данных

**Стратегии балансировки**:
- Регуляризация (L1, L2) для уменьшения variance
- Увеличение сложности модели для уменьшения bias
- Использование ансамблей для балансировки обоих компонентов

### Curse of Dimensionality

**Проблема**: В пространствах высокой размерности:
- Все точки становятся примерно равноудаленными
- Объем данных растет экспоненциально с размерностью
- Многие алгоритмы теряют эффективность

**Решения**:
- Снижение размерности (PCA, t-SNE, autoencoders)
- Feature selection (отбор признаков)
- Использование алгоритмов, устойчивых к высокой размерности (random forests, neural networks)
