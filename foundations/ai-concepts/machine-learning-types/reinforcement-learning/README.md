# Обучение с подкреплением (Reinforcement Learning)

## Описание

Обучение с подкреплением — это тип машинного обучения, при котором агент учится принимать решения, взаимодействуя со средой и получая обратную связь в виде наград или штрафов. Цель агента — максимизировать долгосрочную совокупную награду.

### Математическая формулировка: Марковский процесс принятия решений (MDP)

**Формальное определение MDP**:

MDP = (S, A, P, R, γ)

где:
- **S** — множество состояний (state space)
- **A** — множество действий (action space)
- **P(s'|s, a)** — функция переходов: вероятность перехода в состояние s' из состояния s при действии a
- **R(s, a, s')** — функция награды: ожидаемая награда за переход из s в s' при действии a
- **γ ∈ [0, 1]** — коэффициент дисконтирования (discount factor)

**Цель RL**: Найти оптимальную политику π*: S → A, максимизирующую ожидаемую дисконтированную награду:

π* = argmax_{π} E[∑_{t=0}^∞ γᵗ rₜ | π]

где rₜ — награда в момент времени t.

**Возврат (Return)**:
Gₜ = r_{t+1} + γr_{t+2} + γ²r_{t+3} + ... = ∑_{k=0}^∞ γᵏ r_{t+k+1}

**Частичные награды (Sparse Rewards)**: В RL награды часто редкие и задержанные. Например, в игре награда может быть только в конце игры (победа/поражение), а не на каждом шаге. Это создает проблему credit assignment — как понять, какие действия привели к награде.

## Содержание директории

- [README.md](README.md) - Это описание
- [reinforcement_example.py](reinforcement_example.py) - Практический пример с Q-learning

## Основные характеристики

- **Наличие меток**: Частичные, в форме наград/штрафов
- **Цель**: Максимизация долгосрочной награды
- **Обратная связь**: Через награды от среды
- **Оценка качества**: По суммарной награде за эпизод

## Ключевые концепции

### Агент и среда
- **Агент** - обучающаяся система
- **Среда** - мир, в котором действует агент

### Состояние, действие, награда
- **Состояние** - описание текущей ситуации
- **Действие** - выбор агента, влияющий на среду
- **Награда** - немедленная обратная связь от среды

### Политика и ценность

#### Политика (Policy)

**Политика π** — это стратегия агента, определяющая, какое действие выбрать в каждом состоянии.

**Детерминированная политика**: π: S → A (однозначное соответствие состояние → действие)

**Стохастическая политика**: π(a|s) = P(Aₜ = a | Sₜ = s) (вероятность выбора действия a в состоянии s)

**Оптимальная политика π*** максимизирует ожидаемый возврат:
π* = argmax_{π} E[Gₜ | Sₜ = s, π]

#### Функция ценности состояния (State Value Function)

**V^π(s)** — ожидаемый возврат при следовании политике π из состояния s:

V^π(s) = E_π[Gₜ | Sₜ = s] = E_π[∑_{k=0}^∞ γᵏ r_{t+k+1} | Sₜ = s]

**Уравнение Беллмана для V^π**:
V^π(s) = Σ_a π(a|s) Σ_{s',r} P(s', r|s, a) [r + γV^π(s')]

**Оптимальная функция ценности**:
V*(s) = max_π V^π(s) = max_a Σ_{s',r} P(s', r|s, a) [r + γV*(s')]

#### Функция Q-значения (Action Value Function)

**Q^π(s, a)** — ожидаемый возврат при выборе действия a в состоянии s и последующем следовании политике π:

Q^π(s, a) = E_π[Gₜ | Sₜ = s, Aₜ = a] = E_π[∑_{k=0}^∞ γᵏ r_{t+k+1} | Sₜ = s, Aₜ = a]

**Уравнение Беллмана для Q^π**:
Q^π(s, a) = Σ_{s',r} P(s', r|s, a) [r + γ Σ_{a'} π(a'|s') Q^π(s', a')]

**Оптимальная Q-функция**:
Q*(s, a) = max_π Q^π(s, a) = Σ_{s',r} P(s', r|s, a) [r + γ max_{a'} Q*(s', a')]

**Связь между V* и Q***:
- V*(s) = max_a Q*(s, a)
- π*(s) = argmax_a Q*(s, a)

## Популярные алгоритмы

### Q-learning

**Математическая формулировка**:

Q-learning — алгоритм off-policy для нахождения оптимальной Q-функции.

**Обновление Q-таблицы**:
Q(sₜ, aₜ) ← Q(sₜ, aₜ) + α[r_{t+1} + γ max_{a} Q(s_{t+1}, a) - Q(sₜ, aₜ)]

где:
- α — learning rate (скорость обучения)
- r_{t+1} — полученная награда
- γ — discount factor
- max_{a} Q(s_{t+1}, a) — максимальное Q-значение для следующего состояния

**Теорема о сходимости Q-learning**:

При выполнении условий:
1. Все пары (s, a) посещаются бесконечно часто
2. Learning rate αₜ удовлетворяет условиям Роббинса-Монро:
   - Σₜ αₜ = ∞ (бесконечная сумма)
   - Σₜ αₜ² < ∞ (конечная сумма квадратов)

Q-learning сходится к оптимальной Q-функции Q* с вероятностью 1.

**Гиперпараметры**:
- `learning_rate` (α): скорость обучения (обычно 0.1-0.5, может затухать со временем)
- `discount_factor` (γ): коэффициент дисконтирования (0.9-0.99)
  - Близко к 1: агент заботится о долгосрочных наградах
  - Близко к 0: агент фокусируется на немедленных наградах
- `exploration_rate` (ε): вероятность случайного действия в ε-greedy (обычно 0.1-0.3)
  - Может затухать: εₜ = ε₀ · decay^t
- `num_episodes`: количество эпизодов обучения

**Когда использовать**:
- Дискретные состояния и действия
- Off-policy обучение (может обучаться на данных от другой политики)
- Табличное представление (Q-table) для небольших пространств состояний

### SARSA (State-Action-Reward-State-Action)

**Принцип работы**:

SARSA — алгоритм on-policy, обучающий Q-функцию для текущей политики.

**Обновление**:
Q(sₜ, aₜ) ← Q(sₜ, aₜ) + α[r_{t+1} + γ Q(s_{t+1}, a_{t+1}) - Q(sₜ, aₜ)]

**Отличие от Q-learning**:
- Q-learning: использует max_{a} Q(s_{t+1}, a) (оптимальное действие)
- SARSA: использует Q(s_{t+1}, a_{t+1}) (действие, выбранное текущей политикой)

**Когда использовать**:
- On-policy обучение (обучение политики, которая используется)
- Более консервативное поведение (учитывает реальные действия, а не оптимальные)
- Полезно когда exploration может быть опасным

### Policy Gradient Methods

**Принцип работы**:

Прямая оптимизация политики π_θ(a|s) с параметрами θ.

**Целевая функция**:
J(θ) = E_π[∑_{t=0}^∞ γᵗ rₜ]

**Градиент политики (Policy Gradient Theorem)**:
∇_θ J(θ) = E_π[∇_θ log π_θ(a|s) Q^π(s, a)]

**Алгоритм REINFORCE**:
1. Сгенерировать эпизод, следуя π_θ
2. Вычислить возвраты Gₜ для каждого шага
3. Обновить параметры: θ ← θ + α ∇_θ log π_θ(aₜ|sₜ) Gₜ

**Гиперпараметры**:
- `learning_rate`: скорость обучения (обычно 0.001-0.01)
- `baseline`: базовая линия для уменьшения дисперсии (например, среднее значение награды)

**Когда использовать**:
- Непрерывные пространства действий
- Стохастические политики
- Прямая оптимизация того, что нужно (политики)

### Actor-Critic Methods

**Принцип работы**:

Комбинация Policy Gradient (Actor) и Value Function (Critic).

**Actor**: обновляет политику π_θ
**Critic**: оценивает функцию ценности V^π или Q^π

**Обновление Actor**:
θ ← θ + α_θ ∇_θ log π_θ(a|s) A(s, a)

где A(s, a) = Q(s, a) - V(s) — advantage function

**Обновление Critic**:
w ← w + α_w [r + γV(s') - V(s)] ∇_w V(s)

**Преимущества**:
- Меньшая дисперсия, чем у REINFORCE
- Более стабильное обучение
- Может работать с непрерывными действиями

**Гиперпараметры**:
- `actor_lr`: learning rate для actor (обычно 0.0001-0.001)
- `critic_lr`: learning rate для critic (обычно 0.001-0.01)
- `gamma`: discount factor

### Deep Q-Networks (DQN)

**Принцип работы**:

Q-learning с нейронной сетью для аппроксимации Q-функции.

**Инновации DQN**:
1. **Experience Replay**: хранение и случайная выборка прошлых переходов
2. **Target Network**: отдельная сеть для вычисления целевых значений (стабильность)

**Обновление**:
L(θ) = E[(r + γ max_{a'} Q(s', a'; θ⁻) - Q(s, a; θ))²]

где θ⁻ — параметры target network (обновляются периодически)

**Гиперпараметры**:
- Архитектура сети: количество слоев, нейронов
- `learning_rate`: обычно 0.0001-0.001
- `batch_size`: размер батча для experience replay (32-128)
- `replay_buffer_size`: размер буфера (обычно 10K-1M)
- `target_update_frequency`: частота обновления target network (каждые N шагов)
- `epsilon`: exploration rate (может затухать)

**Когда использовать**:
- Большие пространства состояний (нельзя использовать Q-table)
- Игры, робототехника
- Когда нужна аппроксимация функции

### A3C (Asynchronous Advantage Actor-Critic)

**Принцип работы**:

Асинхронный вариант Actor-Critic с несколькими параллельными агентами.

**Advantage Function**:
A(s, a) = Q(s, a) - V(s) ≈ r + γV(s') - V(s)

**Преимущества**:
- Параллельное обучение (быстрее)
- Разнообразие опыта (разные агенты исследуют разные части пространства)
- Не требует experience replay

**Гиперпараметры**:
- `num_workers`: количество параллельных агентов
- `actor_lr`, `critic_lr`: learning rates
- `entropy_coef`: коэффициент энтропии для поощрения exploration

**Когда использовать**:
- Нужно быстрое обучение
- Доступны вычислительные ресурсы для параллелизма
- Большие пространства состояний

## Когда использовать

Используйте обучение с подкреплением, когда:
- Задача предполагает последовательность решений
- Можно смоделировать среду взаимодействия
- Цель — оптимизация долгосрочного результата

## Exploration vs Exploitation Dilemma

### Проблема

**Exploration (Исследование)**: Попробовать новые действия, чтобы узнать больше о среде
**Exploitation (Использование)**: Использовать известные хорошие действия для максимизации награды

**Компромисс**: Слишком много exploration → медленное обучение, слишком много exploitation → может пропустить лучшие стратегии

### Стратегии решения

#### 1. ε-Greedy

**Принцип**: С вероятностью ε выбирать случайное действие, иначе — лучшее известное действие

π(a|s) = {
  ε/|A| + (1-ε), если a = argmax_a Q(s, a)
  ε/|A|, иначе
}

**Гиперпараметры**:
- `epsilon`: начальная вероятность exploration (0.1-0.3)
- `epsilon_decay`: затухание ε со временем (например, εₜ = ε₀ · 0.995^t)
- `epsilon_min`: минимальное значение ε (0.01-0.05)

#### 2. Upper Confidence Bound (UCB)

**Принцип**: Выбирать действие, максимизирующее верхнюю границу доверительного интервала

aₜ = argmax_a [Q(s, a) + c√(ln(t) / N(s, a))]

где:
- N(s, a) — количество раз, когда действие a было выбрано в состоянии s
- c — параметр exploration (обычно √2)

**Преимущества**: Автоматически балансирует exploration и exploitation

#### 3. Thompson Sampling

**Принцип**: Использовать байесовский подход — выбирать действие согласно вероятности того, что оно оптимально

**Когда использовать**: Когда есть априорные знания о распределении наград

#### 4. Softmax (Boltzmann Exploration)

**Принцип**: Выбирать действие согласно распределению, пропорциональному exp(Q(s, a) / τ)

π(a|s) = exp(Q(s, a) / τ) / Σ_{a'} exp(Q(s, a') / τ)

где τ — температура (больше τ → больше exploration)

## Типичные проблемы и решения

### 1. Sparse Rewards (Редкие награды)

**Проблема**: Награды приходят редко (например, только в конце игры), что замедляет обучение

**Решения**:
- **Reward Shaping**: Добавление промежуточных наград для поощрения желаемого поведения
  - Пример: в игре добавлять небольшую награду за приближение к цели
- **Curriculum Learning**: Начать с простых задач и постепенно усложнять
- **Hindsight Experience Replay (HER)**: Использование "неудачных" попыток как успешных для других целей
- **Intrinsic Motivation**: Добавление внутренней награды за исследование (curiosity-driven learning)

### 2. Нестабильность обучения

**Проблема**: Q-значения могут расходиться или колебаться

**Решения**:
- **Target Network**: Использование отдельной сети для вычисления целевых значений (обновляется реже)
- **Gradient Clipping**: Ограничение градиентов (например, max_norm = 1.0)
- **Learning Rate Scheduling**: Уменьшение learning rate со временем
- **Double Q-Learning**: Использование двух Q-сетей для уменьшения переоценки

### 3. Выбор гиперпараметров

**Критические гиперпараметры**:

1. **Discount Factor (γ)**:
   - 0.9-0.95: для задач с долгосрочными последствиями
   - 0.99: для очень долгосрочных задач
   - 0.5-0.8: для задач с краткосрочными целями

2. **Learning Rate (α)**:
   - 0.1-0.5: для табличных методов (Q-learning)
   - 0.0001-0.001: для нейронных сетей (DQN)
   - Может затухать: αₜ = α₀ / (1 + decay·t)

3. **Exploration Rate (ε)**:
   - Начальное: 1.0 (полное exploration)
   - Конечное: 0.01-0.1 (минимальное exploration)
   - Затухание: экспоненциальное или линейное

**Рекомендации**:
- Начинать с высокого exploration, постепенно уменьшать
- Использовать grid search или random search для оптимизации
- Мониторить метрики (средняя награда, длина эпизода) во время обучения

### 4. Credit Assignment Problem

**Проблема**: Как понять, какие действия привели к награде, особенно при задержанных наградах

**Решения**:
- **Eligibility Traces**: Отслеживание влияния прошлых действий (λ в TD(λ))
- **Temporal Difference Learning**: Обновление на основе разности между последовательными оценками
- **Monte Carlo Methods**: Обучение на полных эпизодах (для эпизодических задач)

### 5. Безопасность во время обучения

**Проблема**: Агент может совершать опасные действия во время обучения

**Решения**:
- **Safe Exploration**: Ограничение пространства действий безопасными
- **Simulation First**: Обучение в симуляции перед применением в реальности
- **Human-in-the-Loop**: Человеческий надзор во время обучения
- **Constrained RL**: Добавление ограничений в задачу оптимизации

### 6. Высокая размерность пространства состояний

**Проблема**: Табличные методы не работают для больших пространств

**Решения**:
- **Function Approximation**: Использование нейронных сетей (DQN, Actor-Critic)
- **Feature Engineering**: Создание компактного представления состояний
- **State Abstraction**: Группировка похожих состояний
- **Attention Mechanisms**: Фокус на релевантных частях состояния

## Метрики оценки качества

### Метрики производительности

1. **Средняя награда за эпизод**:
   - R_avg = (1/N) Σᵢ Rᵢ, где Rᵢ — суммарная награда в эпизоде i

2. **Длина эпизода**:
   - Количество шагов до завершения эпизода
   - Меньше — лучше (для задач с целью)

3. **Success Rate**:
   - Доля успешных эпизодов (достижение цели)

4. **Q-значения**:
   - Мониторинг изменения Q-значений (должны сходиться)

### Метрики обучения

1. **Loss**: Ошибка предсказания Q-значений (для DQN)
2. **Policy Loss**: Градиент политики (для Policy Gradient)
3. **Value Loss**: Ошибка оценки функции ценности (для Critic)

### Визуализация

- **Learning Curves**: График награды от количества эпизодов
- **Q-Table Visualization**: Визуализация Q-значений (для небольших пространств)
- **Policy Visualization**: Визуализация выбранных действий в разных состояниях

## Рекомендации по обучению

### 1. Начальная настройка

1. **Начните с простой среды**: Проверьте алгоритм на простой задаче (например, GridWorld)
2. **Используйте табличные методы**: Для небольших пространств используйте Q-learning с таблицей
3. **Визуализируйте процесс**: Смотрите, как агент обучается

### 2. Переход к сложным задачам

1. **Function Approximation**: Когда таблица становится слишком большой, переходите на нейронные сети
2. **Experience Replay**: Используйте для стабилизации обучения
3. **Target Network**: Обязательно для DQN

### 3. Отладка

1. **Мониторинг метрик**: Следите за наградой, loss, Q-значениями
2. **Проверка exploration**: Убедитесь, что агент исследует достаточно
3. **Визуализация политики**: Смотрите, какие действия выбирает агент

### 4. Оптимизация

1. **Hyperparameter Tuning**: Систематический поиск лучших параметров
2. **Ensemble Methods**: Комбинация нескольких агентов
3. **Transfer Learning**: Использование предобученных моделей

## Практический пример

Запустите [reinforcement_example.py](reinforcement_example.py), чтобы увидеть пример агента, обучающегося перемещаться к цели с помощью Q-learning.